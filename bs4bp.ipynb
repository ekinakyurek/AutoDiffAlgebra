{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(x) = exp(-x)\n",
    "hâ€²(x,y) = -y\n",
    "ğ“(x,y) = sum(abs2,x-y)/2\n",
    "ğ“â€²(x,y) = x-y\n",
    "ğœ€ = .0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{Float64,1},1}:\n",
       " [0.409429, 1.63166]  \n",
       " [1.5489, -0.610416]  \n",
       " [0.0139285, -1.40057]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 3 \n",
    "w = randn(N)\n",
    "b = randn(N)\n",
    "params =[[w[i],b[i]] for i=1:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neural_net (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function neural_net(params, input; h=h, hâ€²=hâ€², N=length(params))\n",
    "    Î´ = [];\n",
    "    X = [input];\n",
    "    for i=1:N\n",
    "        x = params[i]â‹…[X[i],1]\n",
    "        y = h.(x) \n",
    "        push!(Î´, hâ€².(x,y))\n",
    "        push!(X,y)\n",
    "    end\n",
    "    return X,Î´\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{Float64,1},1}:\n",
       " [-0.017232, -0.0742665]\n",
       " [0.0479479, 0.269554]  \n",
       " [-19.3528, -13.8452]   "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = rand()\n",
    "y = 0.5\n",
    "X,Î´   = neural_net(params,x)\n",
    "L     = Bidiagonal(zeros(N),Î´[2:N].*w[2:N],:L)\n",
    "D     = Diagonal(Î´.*[[X[i],1]' for i=1:N])\n",
    "f     = [zeros(N-1);ğ“â€²(X[N+1],y)]\n",
    "âˆ‡J    = D'*((I-L')\\f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{Float64,1},1}:\n",
       " [-0.017232, -0.0742665]\n",
       " [0.0479479, 0.269554]  \n",
       " [-19.3528, -13.8452]   "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "âˆ‡Jfd = âˆ‡J * 0\n",
    "Ïµ    = âˆ‡J * 0\n",
    "for i=1:N, j=1:2       \n",
    "    Ïµ[i][j] = ğœ€\n",
    "    âˆ‡Jfd[i][j]=(ğ“(neural_net(params.+Ïµ,x)[1][N+1],y)-ğ“(neural_net(params.-Ïµ,x)[1][N+1],y))/2ğœ€\n",
    "    Ïµ[i][j] = .0\n",
    "end\n",
    "âˆ‡Jfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Matrix Neural Network\n",
    "- [] needed to create a box type\n",
    "- [] There are some issues with adjoint, did we assume transpose recursive in the article? If it is below code works accordingly.\n",
    "- [] However, adjoint definitions for âŠ—, âŠ—â€² problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "* (generic function with 411 methods)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base: .+,.*,.-,+,-,*,zero,one,adjoint,inv,/,./,convert,size,isequal,iszero,getindex, setindex!\n",
    "\n",
    "abstract type Op; end\n",
    "struct (âŠ—) <: Op; A; B; end\n",
    "struct âŠ—â€² <: Op; A; B; end\n",
    "struct Î”  <: Op; A; end\n",
    "\n",
    "-(K::âŠ—â€²) = -K.A âŠ—â€² K.B \n",
    "*(K::âŠ—â€²,X::Union{AbstractArray,Number}) = K.B * (K.A * X) # changing paranthesis gives wrong result\n",
    "adjoint(K::âŠ—â€²) = K.B âŠ—â€² K.A' # same issue with kronocker adjoint\n",
    "\n",
    "-(K::âŠ—) = -K.A âŠ— K.B\n",
    "*(K::âŠ—,X::Union{AbstractArray,Number}) = (K.B * X) * K.A' # changing paranthesis gives dimension errors\n",
    "adjoint(K::âŠ—) = K.A' âŠ— K.B  # not consistent with adjoint of kroncker, if we keep consistent\n",
    "                            # then the elements of D becomes X[j]' âŠ— Î”(Î´[i])', not consistent with article\n",
    "\n",
    "-(X::Î”) = Î”(-X.A)\n",
    "*(X::Î”,Y::Union{AbstractArray,Number}) = X.A .* Y\n",
    "*(Y::Union{AbstractArray,Number},X::Î”) = Y .* X.A\n",
    "adjoint(X::Î”) = Î”(X.A')\n",
    "\n",
    "# I can't think another way than boxing elements \n",
    "#to handle with zeros and ones requires by the backslash and triangular matrices\n",
    "struct Box; K; end\n",
    "# Unary Definitions\n",
    "zero(::Type{Box}) = Box(0)\n",
    "one(::Type{Box}) = Box(1)\n",
    "zero(::Box) = zero(Box)\n",
    "one(::Box) = one(Box)\n",
    "value(R::Box) = R.K\n",
    "iszero(R::Box) = R.K==0\n",
    "adjoint(R::Box) =Â Box(adjoint(R.K))\n",
    "inv(R::Box) =Â Box(inv(R.K))\n",
    "#Binary Definitions\n",
    "convert(::Type{Box},x::Union{Number,V,T}) where V <: Op where T <: AbstractArray = Box(x)\n",
    "/(X::Number,R::Box) = Box(X*inv(R))\n",
    "/(R1::Box,R2::Box) = Box(R1*inv(R2))\n",
    "-(R::Box) = Box(-R.K)\n",
    "-(R::Box, X::AbstractArray) = Box(R.K-X)\n",
    "-(X::AbstractArray,R::Box)  = Box(X-R.K)\n",
    "-(R1::Box,R2::Box) = Box(R1.K.-R2.K)\n",
    "+(R1::Box,R2::Box) = Box(R1.K.+R2.K)\n",
    "*(R1::Box,R2::Box) = Box(R1.K*R2.K)\n",
    "*(R::Box,X::Union{Number,V,T}) where V <: Op where T <: AbstractArray = Box(R.K * X)\n",
    "*(X::Union{Number,V,T},R::Box) where V <: Op where T <: AbstractArray = Box(X * R.K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array (generic function with 1 method)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Needed to overwrite naivesub!, see comment in 12th line\n",
    "import LinearAlgebra: naivesub!, has_offset_axes\n",
    "function naivesub!(A::UnitUpperTriangular, b::AbstractVector, x::AbstractVector = b)\n",
    "    @assert !has_offset_axes(A, b, x)\n",
    "    n = size(A, 2)\n",
    "    if !(n == length(b) == length(x))\n",
    "        throw(DimensionMismatch(\"second dimension of left hand side A, $n, length of output x, $(length(x)), and length of right hand side b, $(length(b)), must be equal\"))\n",
    "    end\n",
    "    @inbounds for j in n:-1:1\n",
    "        xj = x[j] = b[j]\n",
    "        for i in j-1:-1:1\n",
    "            if !iszero(A.data[i,j]) # EKIN: added this line to solve zero shape related problem. \n",
    "                                    # Otherwise, in matrix neural network we get dimension mismastch\n",
    "                b[i] -= A.data[i,j] * xj\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    x\n",
    "end\n",
    "\n",
    "array(x) = fill(x,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neural_net (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function neural_net(params,input;h=h,hâ€²= hâ€²)\n",
    "    X     = [input]\n",
    "    Î´     = []\n",
    "    for i=1:length(params)\n",
    "        x = params[i][1]*X[i] .+ params[i][2]         \n",
    "        push!(X,h.(x))\n",
    "        push!(Î´,hâ€².(x,X[i+1]))\n",
    "    end \n",
    "    X,Î´\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{Array{Float64,N} where N,1},1}:\n",
       " [[-0.364712 0.144268 1.23095; -0.89484 -0.399452 -0.991036; 0.573454 -0.491818 -0.732605], [-0.275151, 0.433567, -0.21384]]\n",
       " [[0.833144 -0.64884 -0.713167; 1.30563 -0.551628 1.20049; 0.78175 -0.393685 0.156015], [-1.20916, 0.699874, 1.36389]]      \n",
       " [[-0.348164 -0.393493 0.507356], [0.0681898]]                                                                              "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = [3 3 3 1]\n",
    "N = length(n)-1\n",
    "B = 7\n",
    "W = [randn(n[i+1],n[i]) for i=1:N]\n",
    "b = [randn(n[i+1]) for i=1:N]\n",
    "params =[[W[i],b[i]] for i=1:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{Float64,2},1}:\n",
       " [8.36974 11.4023 â€¦ 12.966 90.7926; -2.86048 -4.14977 â€¦ -4.47754 -46.5517; -4.67462 -7.267 â€¦ -9.03337 -105.503]            \n",
       " [-6.4221 -9.27072 â€¦ -10.9274 -105.357; -0.0658502 -0.0854999 â€¦ -0.0927914 -0.675302; 0.265256 0.355902 â€¦ 0.403374 3.04383]\n",
       " [-6.37543 -8.05009 â€¦ -9.04041 -50.748]                                                                                    "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = randn(1,B)*0.1\n",
    "x = randn(n[1],B)*0.1\n",
    "X,Î´ = neural_net(params,x)\n",
    "D = Diagonal([[X[i]' âŠ—  Î”(Î´[i]) Î”(Î´[i]')] for i=1:N])\n",
    "L = Bidiagonal(fill(Box(0),N), [Box(params[i][1] âŠ—â€² Î”(Î´[i])) for i=2:N] , :L)\n",
    "f = [Box(0) for i=1:N]\n",
    "f[N] = ğ“â€²(X[N+1],y)\n",
    "âˆ‡J = D'*array.((UnitUpperTriangular(-L')\\f))\n",
    "âˆ‡Jw = value.(first.(âˆ‡J))\n",
    "âˆ‡Jb = value.(getindex.(âˆ‡J,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{Array{Float64,N} where N,1},1}:\n",
       " [[-13.2795 -5.12737 9.84107; 5.61082 2.92235 -6.03449; 15.0273 6.62905 -13.764], [184.322, -77.7411, -180.31]]     \n",
       " [[-225.501 -121.406 -254.018; -1.58558 -0.838219 -1.73198; 7.19316 3.79558 7.89745], [-188.942, -1.30391, 5.92884]]\n",
       " [[-542.681 -3.31367 -11.6858], [-114.064]]                                                                         "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "âˆ‡Jfd = params*0\n",
    "Ïµ=params*0\n",
    "for i=1:length(params), wb=1:2\n",
    "    for j=1:length(Ïµ[i][wb])\n",
    "            Ïµ[i][wb][j] = ğœ€\n",
    "            âˆ‡Jfd[i][wb][j] =(ğ“(neural_net(params+Ïµ,x)[1][N+1],y)-ğ“(neural_net(params-Ïµ,x)[1][N+1],y))/2ğœ€\n",
    "            Ïµ[i][wb][j] = .0\n",
    "     end\n",
    "end\n",
    "âˆ‡Jfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3Ã—3 Array{Float64,2}:\n",
       " -13.2795   -5.12737    9.84107\n",
       "   5.61082   2.92235   -6.03449\n",
       "  15.0273    6.62905  -13.764  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "âˆ‡Jfd[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3Ã—3 Array{Float64,2}:\n",
       " -13.2795   -5.12737    9.84107\n",
       "   5.61082   2.92235   -6.03449\n",
       "  15.0273    6.62905  -13.764  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "âˆ‡Jw[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       "  184.32190001085758\n",
       "  -77.74109494530279\n",
       " -180.30987600941017"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "âˆ‡Jfd[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3Ã—1 Array{Float64,2}:\n",
       "  184.3218969749331 \n",
       "  -77.7410934801986 \n",
       " -180.30986442829192"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(âˆ‡Jb[1];dims=2) # biasses needs to be sum up in batch dimension, since they are broadcasted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Densely Connected Matrix Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [ ] Defined step matrices to make D'* possible with generic zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "* (generic function with 411 methods)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Horizontal Step Matrix\n",
    "    [---\n",
    "        ---\n",
    "           ---]\n",
    "\n",
    "Vertical Step Matrix\n",
    "    [|\n",
    "     |\n",
    "       |\n",
    "       |\n",
    "       |\n",
    "         |]\n",
    "\"\"\"\n",
    "abstract type StepMatrix{T} <: AbstractMatrix{T}; end;\n",
    "size(S::StepMatrix) = (S.m, S.n)\n",
    "\n",
    "struct VerticalStepMatrix{T} <: StepMatrix{T}\n",
    "    m::Int\n",
    "    n::Int\n",
    "    colptr::Vector{Int}\n",
    "    values::Vector{T}    \n",
    "    function VerticalStepMatrix{T}(m::Int, n::Int, colptr::Vector{Int},values::Vector{T}) where T\n",
    "        m < 0 && error(\"rows m $m\")\n",
    "        n < 0 && error(\"rows n $n\")\n",
    "        new(Int(m), Int(n), colptr, values)\n",
    "    end\n",
    "end\n",
    "\n",
    "getindex(S::VerticalStepMatrix{T},i::Integer, j::Integer) where T = (S.colptr[i] == j ? values[i] : zero(T))\n",
    "setindex!(S::VerticalStepMatrix{T},value::T,i::Integer,j::Integer) where T = (S.colptr[i] = j; values[i] = value)\n",
    "VerticalStepMatrix(t::Type{T},m::Int,n::Int) where T  = VerticalStepMatrix{T}(m, n, Vector{Int}(undef,m),Vector{T}(undef,m))\n",
    "\n",
    "struct HorizontalStepMatrix{T} <: StepMatrix{T}\n",
    "    m::Int\n",
    "    n::Int\n",
    "    rowptr::Vector{Int}\n",
    "    values::Vector{T}    \n",
    "    function HorizontalStepMatrix{T}(m::Int, n::Int, colptr::Vector{Int},values::Vector{T}) where T\n",
    "        m < 0 && error(\"rows m $m\")\n",
    "        n < 0 && error(\"rows n $n\")\n",
    "        new(Int(m), Int(n), colptr, values)\n",
    "    end\n",
    "end\n",
    "getindex(S::HorizontalStepMatrix{T},i::Integer, j::Integer) where T = (S.rowptr[j] == i ? S.values[j] : zero(T))\n",
    "setindex!(S::HorizontalStepMatrix{T},value::T, i::Integer, j::Integer) where T = (S.rowptr[j] = i; S.values[j] = value)\n",
    "HorizontalStepMatrix(t::Type{T},m::Integer, n::Integer) where T  = HorizontalStepMatrix{T}(m, n, Vector{Int}(undef,n),Vector{T}(undef,n))\n",
    "\n",
    "adjoint(S::VerticalStepMatrix{T}) where T = HorizontalStepMatrix{T}(S.n,S.m, S.colptr, adjoint.(S.values))\n",
    "adjoint(S::HorizontalStepMatrix{T}) where T = VerticalStepMatrix{T}(S.n,S.m, S.rowptr, adjoint.(S.values))\n",
    "\n",
    "function (*)(S::VerticalStepMatrix{T}, X::AbstractVector) where T\n",
    "    results = Vector{T}(undef,S.m)\n",
    "    for i=1:S.m\n",
    "        results[i] = S.values[i] * X[S.colptr[i]]\n",
    "    end\n",
    "    return results\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neural_net (generic function with 1 method)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function neural_net(params,input;h=h,hâ€²= hâ€²)\n",
    "    X     = [input]\n",
    "    Î´     = []\n",
    "    layeroffset = 0; i = 1\n",
    "    while layeroffset < length(params)\n",
    "       x = zeros(n[i+1],B)\n",
    "       for j=1:i\n",
    "           x += params[layeroffset+j]*X[j]  \n",
    "       end    \n",
    "       push!(X,h.(x .+ params[layeroffset+i+1]))\n",
    "       push!(Î´,hâ€².(x,X[i+1]))\n",
    "       i+=1; layeroffset+=i\n",
    "    end \n",
    "    X,Î´\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [2 3 2 1]\n",
    "N = length(n)-1\n",
    "B = 7\n",
    "params = []\n",
    "for i=1:N\n",
    "    wi = []\n",
    "    for j=1:i\n",
    "        push!(params,randn(n[i+1],n[j]))\n",
    "    end\n",
    "    push!(params,randn(n[i+1]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9-element Array{Array{Float64,2},1}:\n",
       " [6.55357e-7 -5.60905e-7; 5.52284e-6 -3.8049e-6; -3.02292e-7 2.33001e-7]                                                                        \n",
       " [-1.4427e-8 2.67441e-6 â€¦ 5.84388e-9 1.22249e-6; -1.18827e-7 1.9039e-5 â€¦ 5.60568e-8 7.0659e-6; 5.96991e-9 -1.15829e-6 â€¦ -2.36938e-9 -4.57883e-7]\n",
       " [-1.4041e-7 1.05284e-7; 5.60857e-6 -4.30496e-6]                                                                                                \n",
       " [-1.58873e-6 -8.97747e-7 -4.38173e-7; 6.42688e-5 3.61923e-5 1.76933e-5]                                                                        \n",
       " [3.70509e-9 -5.04579e-7 â€¦ -1.92665e-9 -2.19364e-7; -1.28588e-7 2.09442e-5 â€¦ 5.89779e-8 8.79205e-6]                                             \n",
       " [-4.52938e-7 3.33163e-7]                                                                                                                       \n",
       " [-5.12828e-6 -2.91306e-6 -1.41792e-6]                                                                                                          \n",
       " [-1.05468e-6 -1.85894e-5]                                                                                                                      \n",
       " [9.30544e-9 -1.67356e-6 â€¦ -3.944e-9 -6.33433e-7]                                                                                               "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = randn(1,B)*0.1\n",
    "x = randn(n[1],B)*0.1\n",
    "X,Î´ = neural_net(params,x)\n",
    "D = HorizontalStepMatrix(Box,N,sum(2:N+1))\n",
    "L = LowerTriangular(zeros(Box,N,N))\n",
    "for i=1:N\n",
    "    layeroffset = sum(2:i)\n",
    "    for j=1:i\n",
    "        D[i,layeroffset+j] = Box(X[j]' âŠ— Î”(Î´[i]))\n",
    "        if i>1 && j!=i\n",
    "            L[i,j] = Box(params[layeroffset+j+1] âŠ—â€² Î”(Î´[i]))\n",
    "        end\n",
    "    end\n",
    "    D[i,layeroffset+i+1] = Box(Î”(Î´[i]'))\n",
    "end\n",
    "\n",
    "f = [Box(0) for i=1:N] # [zeros(n[i+1],B) for i=1:N] may use this without modifying naivesub!. However,matrix neural network won't work \n",
    "f[N] = ğ“â€²(X[N+1],y)\n",
    "\n",
    "âˆ‡J=D'*(UnitUpperTriangular(-L')\\f) # Trick: I-L' = UnitUpperTriangular(-L')\n",
    "âˆ‡J=value.(âˆ‡J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9-element Array{Array{Float64,N} where N,1}:\n",
       " [6.55357e-7 -5.60905e-7; 5.52284e-6 -3.8049e-6; -3.02292e-7 2.33e-7]   \n",
       " [5.77296e-6, 4.60926e-5, -2.59625e-6]                                  \n",
       " [-1.4041e-7 1.05284e-7; 5.60857e-6 -4.30496e-6]                        \n",
       " [-1.58873e-6 -8.97747e-7 -4.38173e-7; 6.42688e-5 3.61923e-5 1.76933e-5]\n",
       " [-1.186e-6, 4.79291e-5]                                                \n",
       " [-4.52938e-7 3.33163e-7]                                               \n",
       " [-5.12828e-6 -2.91306e-6 -1.41792e-6]                                  \n",
       " [-1.05468e-6 -1.85894e-5]                                              \n",
       " [-3.84028e-6]                                                          "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "âˆ‡Jfd = params*0\n",
    "Ïµ=params*0\n",
    "for i=1:length(params)\n",
    "    for j=1:length(Ïµ[i])\n",
    "            Ïµ[i][j] = ğœ€\n",
    "            âˆ‡Jfd[i][j] =(ğ“(neural_net(params+Ïµ,x)[1][N+1],y)-ğ“(neural_net(params-Ïµ,x)[1][N+1],y))/2ğœ€\n",
    "            Ïµ[i][j] = .0\n",
    "     end\n",
    "end\n",
    "âˆ‡Jfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3Ã—2 Array{Float64,2}:\n",
       "  6.55357e-7  -5.60905e-7\n",
       "  5.52284e-6  -3.8049e-6 \n",
       " -3.02292e-7   2.33001e-7"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "âˆ‡J[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3Ã—2 Array{Float64,2}:\n",
       "  6.55357e-7  -5.60905e-7\n",
       "  5.52284e-6  -3.8049e-6 \n",
       " -3.02292e-7   2.33e-7   "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "âˆ‡Jfd[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3Ã—7 Array{Float64,2}:\n",
       " -1.4427e-8    2.67441e-6  -4.91386e-8  â€¦   5.84388e-9   1.22249e-6\n",
       " -1.18827e-7   1.9039e-5   -5.46281e-7      5.60568e-8   7.0659e-6 \n",
       "  5.96991e-9  -1.15829e-6   2.04782e-8     -2.36938e-9  -4.57883e-7"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "âˆ‡J[2] # biasses needs to be sum up in batch dimension, since they are broadcasted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3Ã—1 Array{Float64,2}:\n",
       "  5.772956965034854e-6\n",
       "  4.609262547142157e-5\n",
       " -2.59624744148595e-6 "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "âˆ‡J[2] = sum(âˆ‡J[2];dims=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       "  5.772956904182003e-6 \n",
       "  4.609263155180843e-5 \n",
       " -2.5962474531349145e-6"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "âˆ‡Jfd[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.7.0",
   "language": "julia",
   "name": "julia-0.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
