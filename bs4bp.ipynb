{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\\ (generic function with 152 methods)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "import Base: \\\n",
    "function LinearAlgebra.Bidiagonal(dv::Vector{T}, ev::Vector{S}, uplo::Symbol) where {T,S}\n",
    "    TS = promote_type(T,S)\n",
    "    return Bidiagonal{TS,Vector{TS}}(dv, ev, uplo)\n",
    "end\n",
    "\n",
    "\n",
    "## The base method narrows the type too much. We'll have to ensure that it's as least as wide as the input\n",
    "function  \\(adjA::Adjoint{<:Any,<:Union{UnitUpperTriangular,UnitLowerTriangular}}, B::AbstractVector)\n",
    "    A = adjA.parent\n",
    "    TAB = promote_type(eltype(A), eltype(B), typeof(zero(eltype(A))*zero(eltype(B)) + zero(eltype(A))*zero(eltype(B))))\n",
    "    BB = similar(B, TAB, size(B))\n",
    "    copyto!(BB, B)\n",
    "    ldiv!(adjoint(convert(AbstractArray{TAB}, A)), BB)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init (generic function with 1 method)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(x) = exp(-x)\n",
    "h′(x,y) = -y\n",
    "𝓁(x,y) = sum(abs2,x-y)/2\n",
    "𝓁′(x,y) = x-y\n",
    "init(sizes...) = 0.01randn(sizes...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "𝜀 = .0001\n",
    "n = [5,4,3,1]\n",
    "N = length(n)-1\n",
    "B = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neural_net (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function neural_net(params, input; h=h, h′=h′, N=length(params))\n",
    "    δ = [];\n",
    "    X = [input];\n",
    "    for i=1:N\n",
    "        x = sum(params[i] .* [X[i],1])\n",
    "        push!(X,h(x))\n",
    "        push!(δ, h′.(x,X[i+1]))\n",
    "    end\n",
    "    return X,δ\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09969491180448342, 0.17669682675943965)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params =[[init(),init()] for i=1:N] # W and B\n",
    "x,y = init(),init() # input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{Float64,1},1}:\n",
       " [0.000332745, 0.00333763]\n",
       " [0.0391587, 0.0434531]   \n",
       " [-0.585641, -0.560265]   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,δ = neural_net(params,x)\n",
    "L   = Bidiagonal(zeros(N),[δ[i] * params[i][1] for i=2:N],:L)\n",
    "D   = Diagonal(δ.*[[X[i],1]' for i=1:N])\n",
    "g   = [zeros(N-1);𝓁′(X[N+1],y)]\n",
    "∇J  = D'*((I-L')\\g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{Float64,1},1}:\n",
       " [0.000332745, 0.00333763]\n",
       " [0.0391587, 0.0434531]   \n",
       " [-0.585642, -0.560265]   "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ∇Jfd is gradient calculated with finite differences method\n",
    "∇Jfd = ∇J * 0\n",
    "ϵ    = ∇J * 0\n",
    "for i=1:N, j=1:2       \n",
    "    ϵ[i][j] = 𝜀\n",
    "    ∇Jfd[i][j]=(𝓁(neural_net(params.+ϵ,x)[1][N+1],y)-𝓁(neural_net(params.-ϵ,x)[1][N+1],y))/2𝜀\n",
    "    ϵ[i][j] = .0\n",
    "end\n",
    "∇Jfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "* (generic function with 349 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base: +,-,*,/,∘\n",
    "\n",
    "struct LinearMatrixOp # Is parametric type necessary? It causes un-readable error messages and some other issues.\n",
    "    f\n",
    "    fadj\n",
    "end\n",
    "LinearMatrixOp(f::Function) = LinearMatrixOp(f,f)\n",
    "\n",
    "LeftMul(A::Matrix) = LinearMatrixOp(X->A*X, X->A'*X)\n",
    "RightMul(A::Matrix) = LinearMatrixOp(X->X*A, X->X*A')\n",
    "HadMul(A::Matrix) = LinearMatrixOp(X->X.*A)\n",
    "ZeroMul() = LinearMatrixOp(X->Zero())\n",
    "IdentMul() = LinearMatrixOp(X->X) #not neccessary, can be commented\n",
    "\n",
    "Base.zero(::Type{LinearMatrixOp}) = ZeroMul() \n",
    "Base.one(::Type{LinearMatrixOp}) = IdentMul()\n",
    "Base.adjoint(A::LinearMatrixOp) = LinearMatrixOp(A.fadj,A.f)\n",
    "Base.copy(A::LinearMatrixOp) =  LinearMatrixOp(A.f,A.fadj)\n",
    "\n",
    "*(A::LinearMatrixOp,X::Union{AbstractArray,Number}) = A.f(X)\n",
    "-(A::LinearMatrixOp) = LinearMatrixOp(X->-A.f(X), X->-A.fadj(X))\n",
    "∘(A::LinearMatrixOp, B::LinearMatrixOp) = LinearMatrixOp(A.f ∘ B.f, B.fadj ∘ A.fadj)\n",
    "\n",
    "# A zero\n",
    "struct Zero end\n",
    "Base.zero(::Type{Any}) = Zero()\n",
    "+(::Zero, ::Zero) = Zero()\n",
    "-(::Zero, A) = -A\n",
    "+(::Zero, A) = A\n",
    "*(::Zero, ::Zero) = Zero()\n",
    "*(X, ::Zero) = Zero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function neural_net(params,input;h=h,h′= h′)\n",
    "    X     = [input]\n",
    "    δ     = []\n",
    "    for i=1:length(params)\n",
    "        x = params[i][1]*X[i] .+ params[i][2]         \n",
    "        push!(X,h.(x))\n",
    "        push!(δ,h′.(x,X[i+1]))\n",
    "    end \n",
    "    X,δ\n",
    "end\n",
    "array(x)= fill(x,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params: `W_i` and `b_i`s: x_{i+1} <- Wi*x_i .+ b_i\n",
    "params =[[init(n[i+1],n[i]),init(n[i+1])] for i=1:N]\n",
    "x, y = init(n[1],B), init(1,B);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,δ = neural_net(params,x)\n",
    "D = Diagonal([[HadMul(δ[i]) ∘ RightMul(X[i]) HadMul(δ[i])] for i=1:N])\n",
    "ImL = Bidiagonal([I for i in 1:N], -[HadMul(δ[i]) ∘ LeftMul(params[i][1]) for i=2:N] , :L)\n",
    "g = push!(Any[Zero() for i=1:N-1],𝓁′(X[N+1],y))\n",
    "∇J = D'*array.(ImL'\\g);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ∇Jfd is gradient calculated with finite differences method\n",
    "∇Jfd = params*0\n",
    "ϵ=params*0\n",
    "for i=1:length(params), wb=1:2\n",
    "    for j=1:length(ϵ[i][wb])\n",
    "        ϵ[i][wb][j] = 𝜀\n",
    "        ∇Jfd[i][wb][j] =(𝓁(neural_net(params+ϵ,x)[1][N+1],y)-𝓁(neural_net(params-ϵ,x)[1][N+1],y))/2𝜀\n",
    "        ϵ[i][wb][j] = .0\n",
    "     end\n",
    "end\n",
    "∇Jfd;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×5 Array{Float64,2}:\n",
       " -0.00918551   0.0105151   -0.0103996    0.0021086   -0.00231879\n",
       "  0.0078814   -0.00946623   0.010044    -0.00155979   0.0025162 \n",
       "  0.00581915  -0.00715716   0.00616917  -0.00195801   0.00147022\n",
       "  0.0050518   -0.00622673   0.00536373  -0.00161102   0.00129137"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇Jfd[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×5 Array{Float64,2}:\n",
       " -0.00918551   0.0105151   -0.0103996    0.0021086   -0.00231879\n",
       "  0.0078814   -0.00946623   0.010044    -0.00155979   0.0025162 \n",
       "  0.00581915  -0.00715716   0.00616917  -0.00195801   0.00147022\n",
       "  0.0050518   -0.00622673   0.00536373  -0.00161102   0.00129137"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇J[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Showcase: Densely Connected Matrix Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "function neural_net(params,input;h=h, h′= h′)\n",
    "    X     = [input]\n",
    "    δ     = []\n",
    "    for i in 1:length(params)\n",
    "       x = broadcast(+,(params[i] .* [X..., I])...)\n",
    "       push!(X,h.(x))\n",
    "       push!(δ,h′.(x,X[i+1]))\n",
    "    end \n",
    "    X,δ\n",
    "end;\n",
    "array(x) = fill(x,1,1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [[j==i+1 ?  init(n[i+1],1) : init(n[i+1],n[j])  for j=1:i+1] for i=1:N]\n",
    "x,y = init(n[1],B), init(1,B);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{Any,2},1}:\n",
       " [[-0.0146373 -0.0171189 … -0.0210885 0.0410024; -0.0143086 -0.018308 … -0.0222181 0.0421256; -0.00966715 -0.0121301 … -0.0141614 0.0269208; 0.00152569 0.00182626 … 0.00191509 -0.00355642]; [-0.10794 -0.0975917 … -0.118599 -0.103915; -0.108819 -0.0990499 … -0.121469 -0.11092; -0.0704849 -0.0637224 … -0.0766635 -0.0709802; 0.0094296 0.00829117 … 0.009679 0.00939663]]                         \n",
       " [[0.0142146 0.0169136 … 0.0204834 -0.0398483; 0.0325147 0.0388068 … 0.0432487 -0.082327; -0.000177677 -0.000228124 … -0.000263707 0.000485406]; [0.720403 0.635437 0.737404 0.637144; 1.49741 1.32101 1.53354 1.32525; -0.00880898 -0.00777354 -0.00902285 -0.00779906]; [0.104437 0.0943859 … 0.114782 0.101125; 0.218172 0.194104 … 0.230717 0.213583; -0.0012477 -0.0011209 … -0.0013769 -0.0012992]]\n",
       " [[-0.168753 -0.210445 … -0.249917 0.47049]; [-8.52873 -7.52481 -8.73304 -7.54717]; [-10.1459 -10.852 -6.52011]; [-1.22108 -1.10112 … -1.34801 -1.23476]]                                                                                                                                                                                                                                                "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,δ = neural_net(params,x)\n",
    "D = Diagonal([[[(HadMul(δ[i]) ∘ RightMul(X[j]))' for j=1:i]' HadMul(δ[i])] for i=1:N])\n",
    "ImL = UnitLowerTriangular(Matrix{Any}(undef,N,N))\n",
    "for i=2:N, j=1:i-1\n",
    "    ImL[i,j] = -HadMul(δ[i]) ∘ LeftMul(params[i][j+1]) \n",
    "end\n",
    "g = push!(Any[Zero() for i=1:N-1],𝓁′(X[N+1],y))\n",
    "∇J = D'*array.(ImL'\\g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ∇Jfd is gradient calculated with finite differences method\n",
    "∇Jfd = params*0\n",
    "ϵ=params*0\n",
    "for i=1:length(ϵ), j=1:length(ϵ[i]), k=1:length(ϵ[i][j])\n",
    "        ϵ[i][j][k] = 𝜀\n",
    "        ∇Jfd[i][j][k] =(𝓁(neural_net(params+ϵ,x)[1][N+1],y)-𝓁(neural_net(params-ϵ,x)[1][N+1],y))/2𝜀\n",
    "        ϵ[i][j][k] = .0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×5 Array{Float64,2}:\n",
       " -0.0146373   -0.0171189   -0.0466687   -0.0210885    0.0410024 \n",
       " -0.0143086   -0.018308    -0.0482024   -0.0222181    0.0421256 \n",
       " -0.00966715  -0.0121301   -0.0302245   -0.0141614    0.0269208 \n",
       "  0.00152569   0.00182626   0.00356372   0.00191509  -0.00355642"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇Jfd[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×5 Array{Float64,2}:\n",
       " -0.0146373   -0.0171189   -0.0466687   -0.0210885    0.0410024 \n",
       " -0.0143086   -0.018308    -0.0482024   -0.0222181    0.0421256 \n",
       " -0.00966715  -0.0121301   -0.0302245   -0.0141614    0.0269208 \n",
       "  0.00152569   0.00182626   0.00356372   0.00191509  -0.00355642"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇J[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST MLP Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neural_net (generic function with 1 method)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "using Knet\n",
    "import Knet: Data\n",
    "include(Knet.dir(\"data\",\"mnist.jl\"))\n",
    "dtrn,dtst = mnistdata(xsize=(784,:)); # dtrn and dtst = [ (x1,y1), (x2,y2), ... ] where xi,yi are\n",
    "\n",
    "#Layers\n",
    "n = [784,128,64,10]\n",
    "N = length(n)-1\n",
    "init(sizes...) = 0.1randn(sizes...)\n",
    "\n",
    "#Nonlinearity\n",
    "h(x)    = x>0 ? x : zero(x) # relu\n",
    "h′(x,y) = y>0 ? one(x) : zero(x) # derivative of relu\n",
    "\n",
    "#Loss\n",
    "𝓁(x,a) = nll(x,a;average=true) # negative log likelihood loss, x is dxb matrix, \n",
    "                               # a is d-length integer array keeps the correct answers \n",
    "function 𝓁′(x,a)  # Note!: this will be simplified if we can figure out how to integrate derivative of getindex in to our formulatin\n",
    "    indices = Knet.findindices(x,a,dims=1)\n",
    "    yz = zero(x)\n",
    "    yz[indices] .= 1\n",
    "    return (softmax(x,dims=1) .- yz)./length(a)\n",
    "end\n",
    "\n",
    "#Forward Function\n",
    "function neural_net(params,input;h=h,h′= h′)\n",
    "    X     = [input]\n",
    "    δ     = []\n",
    "    for i=1:length(params)-1\n",
    "        x = params[i][1]*X[end] .+ params[i][2]         \n",
    "        push!(X,h.(x)); push!(δ,h′.(x,X[end]))\n",
    "    end \n",
    "    x = params[end][1]*X[end] .+ params[end][2]    \n",
    "    push!(X,x)\n",
    "    push!(δ,one.(x))\n",
    "    X,δ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params =[[init(n[i+1],n[i]),zeros(n[i+1])] for i=1:N] # model parameters\n",
    "α = 0.5 # learning rate \n",
    "epochs=3# number of epochs to train model \n",
    "@time for i=1:epochs # 1 epoch takes ~ 65 seconds  in my macbook\n",
    "    for (x,y) in dtrn\n",
    "        X,δ = neural_net(params,x;h=h, h′= h′)\n",
    "        D = Diagonal([[HadMul(δ[i]) ∘ RightMul(X[i]) HadMul(δ[i])] for i=1:N])\n",
    "        ImL = Bidiagonal([I for i in 1:N], -[HadMul(δ[i]) ∘ LeftMul(params[i][1]) for i=2:N] , :L)\n",
    "        g = push!(Any[Zero() for i=1:N-1],𝓁′(X[N+1],y))\n",
    "        ∇J = D'*array.(ImL'\\g);\n",
    "        for i =1:length(params)\n",
    "            params[i][1] = params[i][1] - α*∇J[i][1]\n",
    "            params[i][2] = params[i][2] - α*sum(∇J[i][2],dims=2)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroone=total=0\n",
    "for (x,y) in dtst\n",
    "    yn        = neural_net(params,x;h=h, h′= h′)[1][end]\n",
    "    answers   = vec(getindex.(argmax(yn,dims=1),1))\n",
    "    global zeroone += sum(y .== answers)\n",
    "    global total   += length(answers)\n",
    "end\n",
    "accuracy = 100zeroone/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
